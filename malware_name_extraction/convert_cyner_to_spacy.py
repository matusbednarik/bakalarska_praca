import spacy
from spacy.tokens import DocBin
from datasets import load_dataset

def convert_cyner_to_spacy(target_entity="Malware"):
    """
    Convert CyNER 2.0 dataset to spaCy format for training.
    
    Args:
        target_entity: Entity type to extract (default: "Malware")
    """
    # Load dataset
    print("Loading CyNER 2.0 dataset...")
    dataset = load_dataset("PranavaKailash/CyNER2.0_augmented_dataset")
    
    # Process each split
    for split_name in ["train", "validation", "test"]:
        print(f"\nProcessing {split_name} split...")
        
        nlp = spacy.blank("en")
        doc_bin = DocBin()
        processed_count = 0
        
        for example in dataset[split_name]:
            tokens = example["tokens"]
            ner_tags = example["ner_tags"]
            
            # Convert integer tags to strings if needed
            if isinstance(ner_tags[0], int):
                tag_feature = dataset[split_name].features["ner_tags"].feature
                ner_tags = [tag_feature.int2str(tag) for tag in ner_tags]
            
            # Reconstruct text and find entities
            text = " ".join(tokens)
            entities = extract_entities(tokens, ner_tags, target_entity)
            
            # Skip if no target entities found
            if not entities:
                continue
            
            # Create spaCy doc
            doc = nlp.make_doc(text)
            spacy_entities = []
            
            for start_char, end_char, label in entities:
                span = doc.char_span(start_char, end_char, label=label, alignment_mode="contract")
                if span:
                    spacy_entities.append(span)
            
            if spacy_entities:
                doc.ents = spacy_entities
                doc_bin.add(doc)
                processed_count += 1
        
        # Save to file
        output_file = f"{target_entity.lower()}_{split_name}.spacy"
        doc_bin.to_disk(output_file)
        print(f"Saved {processed_count} examples to {output_file}")

def extract_entities(tokens, ner_tags, target_entity):
    """
    Extract character-level entity spans from BIO-tagged tokens.
    
    Returns:
        List of (start_char, end_char, label) tuples
    """
    entities = []
    text = ""
    current_entity_start = None
    
    for i, (token, tag) in enumerate(zip(tokens, ner_tags)):
        token_start = len(text)
        
        # Add space before token (except first token)
        if i > 0:
            text += " "
            token_start = len(text)
        
        # Check if this token starts or continues our target entity
        if tag == f"B-{target_entity}":
            # Start new entity
            current_entity_start = token_start
        elif tag == f"I-{target_entity}" and current_entity_start is not None:
            # Continue current entity (do nothing)
            pass
        else:
            # End current entity if we were tracking one
            if current_entity_start is not None:
                entities.append((current_entity_start, token_start - 1, target_entity))
                current_entity_start = None
        
        text += token
    
    # Handle entity that goes to end of text
    if current_entity_start is not None:
        entities.append((current_entity_start, len(text), target_entity))
    
    return entities

if __name__ == "__main__":
    convert_cyner_to_spacy("Malware") 